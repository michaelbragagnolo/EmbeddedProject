{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4558ab",
   "metadata": {},
   "source": [
    "***\n",
    "## Generative Model - VAE\n",
    "\n",
    "Experiment implementing a **MultiLayer Perceptron Variational AutoEncoder** as generative model of data needed for the Generative Replay technique.\n",
    "\n",
    "The Variational AutoEncoder (VAE) is an architecture composed of an encoder, a decoder and a loss function, that is trained to minimize the reconstruction error between the encoded-decoded data and the initial data.\n",
    "\n",
    "*Code is based in part on the work:*\n",
    " - https://github.com/TLESORT/Generative_Continual_Learning \n",
    " - https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#vae\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67abfae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LIBRARIES AND UTILS ---\n",
    "from abc import abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "import avalanche\n",
    "from avalanche.models import MLP, Flatten, BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60952c35",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "Encoder part of the VAE, it is a fully connected neural network that takes in an input and generates a much smaller, dense representation (encoding) specifically useful for reconstructing its own input by mean of a decoder.\n",
    "\n",
    "#### Input parameters:\n",
    " - shape = shape of the network input (1, height, width)\n",
    " - latent_dim = dimension of the last hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e56621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, shape, latent_dim=128):\n",
    "        super(VAE_encoder, self).__init__()\n",
    "        flattened_size = torch.Size(shape).numel()\n",
    "        self.encode = nn.Sequential(\n",
    "            Flatten(),      # nn.Module to flatten each tensor of a batch of tensors\n",
    "            nn.Linear(in_features=flattened_size, out_features=400), # single layer MLP\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.LeakyReLU(), # no vanishing gradient\n",
    "            MLP([400, latent_dim], last_activation=True), # MLP with BatchNorm and Relu activations\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.encode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b2fd5",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "Decoder part of the VAE, it has the same network structure of the encoder; it takes the output of the encoder and attempts to recreate an output identical to the input.\n",
    "\n",
    "#### Input parameters:\n",
    " - shape = shape of the network output (1, height, width)\n",
    " - nhid = dimension of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9973943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, nhid=16):\n",
    "        super(VAE_decoder, self).__init__()\n",
    "        flattened_size = torch.Size(shape).numel()\n",
    "        self.shape = shape\n",
    "        self.decode = nn.Sequential(\n",
    "            MLP([nhid, 64, 128, 256, flattened_size], last_activation=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.invTrans = Compose([Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def forward(self, z, y=None):\n",
    "        if y is None:\n",
    "            return self.invTrans(self.decode(z).view(-1, *self.shape))\n",
    "        else:\n",
    "            return self.invTrans(\n",
    "                self.decode(torch.cat((z, y), dim=1)).view(-1, *self.shape)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be2c8c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Variational AutoEncoder module and loss\n",
    "Fully connected Variational Autoencoder\n",
    "\n",
    "#### Input parameters:\n",
    " - shape = shape of each input sample\n",
    " - nhid = dimension of the latent space of the Encoder\n",
    " - num_classes = number of classes, it defines the classification head's dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051cd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### base abstract class for generators\n",
    "class Generator(BaseModel):\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, batch_size=None, condition=None):\n",
    "        \"\"\"\n",
    "        Lets the generator sample random samples.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class VAE_model(Generator, nn.Module):\n",
    "\n",
    "    def __init__(self, shape, nhid=16, n_classes=10, device=\"cpu\"):\n",
    "        \n",
    "        super(VAE_model, self).__init__()\n",
    "        self.dim = nhid\n",
    "        self.device = device\n",
    "        self.encoder = VAE_encoder(shape, latent_dim=128)\n",
    "        self.calc_mean = MLP([128, nhid], last_activation=False)\n",
    "        self.calc_logvar = MLP([128, nhid], last_activation=False)\n",
    "        self.classification = MLP([128, n_classes], last_activation=False)\n",
    "        self.decoder = VAE_decoder(shape, nhid)\n",
    "\n",
    "    # Get features for encoder part given input x\n",
    "    def get_features(self, x):\n",
    "\n",
    "        return self.encoder(x)\n",
    "\n",
    "    # Random samples generator\n",
    "    def generate(self, batch_size=None): # :param batch_size = samples to generate\n",
    "\n",
    "        z = (torch.randn((batch_size, self.dim)).to(self.device) # batch of samples of size \"batch_size\"\n",
    "            if batch_size\n",
    "            else torch.randn((1, self.dim)).to(self.device))     # if batch_size = None, single sample\n",
    "        \n",
    "        res = self.decoder(z)\n",
    "        \n",
    "        if not batch_size:\n",
    "            res = res.squeeze(0)\n",
    "        return res\n",
    "    \n",
    "    # Reparametrization trick\n",
    "    def sampling(self, mean, logvar):\n",
    "        \n",
    "        eps = torch.randn(mean.shape).to(self.device)\n",
    "        sigma = 0.5 * torch.exp(logvar)\n",
    "        return mean + eps * sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        representations = self.encoder(x)\n",
    "        mean, logvar = self.calc_mean(representations), self.calc_logvar(representations)\n",
    "        z = self.sampling(mean, logvar)\n",
    "        return self.decoder(z), mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2a043",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Loss function of VAE using mean squared error for reconstruction loss (used for VAE training)\n",
    "\n",
    "#### Input parameters:\n",
    " - X = input batch\n",
    " - forward_output = [reconstructed input after autoencoder, mean of the VAE output distribution, logvar of the VAE output distribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b63ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss = nn.MSELoss(reduction=\"sum\")# Criterion that measures mean squared error (L2 norm)\n",
    "\n",
    "def VAE_loss(X, forward_output):\n",
    "\n",
    "    X_hat, mean, logvar = forward_output\n",
    "    reconstruction_loss = MSE_loss(X_hat, X)\n",
    "    KL_divergence = 0.5 * torch.sum(-1 - logvar + torch.exp(logvar) + mean ** 2)\n",
    "    return reconstruction_loss + KL_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02331923",
   "metadata": {},
   "source": [
    "***\n",
    "List of strings defining what symbols are exported in the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a666e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"VAE_model\", \"VAE_loss\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
