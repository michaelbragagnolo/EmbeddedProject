## Description

### INTRO
**Deep generative replay framework** that generates fake inputs from learned past input distribution to retain knowledge without revisiting actual past data. The network in fact is jointly optimized using an ensemble of generated past data and real current data.

Performance depend on the quality of the generator and its ability to recover the input distribution.

### 1. Generative Model
Hereby it is considered a variational autoencoder deep generative model based on a fully connected neural network that is able to mimic complex samples like images. The term generative model refers to any model that generates observable samples.

**The Variational AutoEncoder (VAE)** is an architecture composed of an encoder, a decoder and a loss function, that is trained to minimize the reconstruction error between the encoded-decoded data and the initial data:
<div align="center">

<img src="autoencoder.png" alt="drawing" style="width:400px;"/>
</div> 

Reference script: *GenerativeModel.py*. Any kind of generative model can be used, the script utilizes a VAE generator.

### 2. Generator
The **Generator** for the scholar model is wrapped into a trainable strategy, to be passed to the generator_strategy parameter when using Generative Replay plugin of Avalanche.

Reference script: *Generator.py*. Script serves as a test to visualize the samples generated by the VAE generative model.

### 3. Training strategies
Instances of the **training strategies** for the VAE generative model and Generative Replay, based on the SupervisedTemplate of Avalanche, which is a basic training loop and callback system that allows to execute code at each experience of the training loop.

Reference script: *TrainingStrategies.py*.

### 4. Generative Replay
Experiment reproducing Generative Replay strategy: Deep Generative Replay for a Scholar consisting of a Solver and Generator.

<div align="center">

<img src="scolar.png" alt="drawing" style="width:400px;"/>
</div> 

The scholar model is built with a solver that has suitable architecture for solving a task sequence and a generator trained using a variational autoencoder. However, this framework can employ any deep generative model as a generator.

