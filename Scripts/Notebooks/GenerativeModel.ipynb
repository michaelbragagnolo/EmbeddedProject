{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965fa068",
   "metadata": {},
   "source": [
    "***\n",
    "## Generative Model - VAE\n",
    "\n",
    "Experiment implementing a **fully-connected Variational AutoEncoder** as generative model of data needed for the Generative Replay technique.\n",
    "\n",
    "The Variational AutoEncoder (VAE) is an architecture composed of an encoder, a decoder and a loss function, that is trained to minimize the reconstruction error between the encoded-decoded data and the initial data.\n",
    "\n",
    "*Code is based in part on the work:*\n",
    " - https://blog.keras.io/building-autoencoders-in-keras.html\n",
    " - https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#vae\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c59d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LIBRARIES AND UTILS ---\n",
    "from abc import abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "import avalanche\n",
    "from avalanche.models import MLP, Flatten, BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcf4bc",
   "metadata": {},
   "source": [
    "***\n",
    "### Encoder\n",
    "Encoder part of the VAE, it is a fully connected neural network that takes in an input and generates a much smaller, dense representation (encoding) specifically useful for reconstructing its own input by mean of a decoder.\n",
    "\n",
    "#### Input parameters:\n",
    " - shape = shape of the network input (channels, height, width) of the handled images\n",
    " - latent_dim = dimension of the latent space Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb07ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, shape, latent_dim=128):\n",
    "        super(VAE_encoder, self).__init__()\n",
    "        flattened_size = torch.Size(shape).numel() # original input shape\n",
    "        self.encode = nn.Sequential(\n",
    "            Flatten(),      # nn.Module to flatten each tensor of a batch of tensors\n",
    "            nn.Linear(in_features=flattened_size, out_features=400), # 1st layer (image -> 400 units)\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.LeakyReLU(), # no vanishing gradient\n",
    "            MLP([400, latent_dim], last_activation=True), # 2nd layer (400 units -> Z latent space)\n",
    "        )\n",
    "\n",
    "    # Encoder maps an input-vector X to a vector of latent variables Z \n",
    "    def forward(self, x, y=None):\n",
    "        x = self.encode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893969e",
   "metadata": {},
   "source": [
    "***\n",
    "### Decoder\n",
    "Decoder part of the VAE, it has the same network structure of the encoder; it takes the output of the encoder and attempts to recreate an output identical to the input.\n",
    "\n",
    "#### Input parameters:\n",
    " - shape = shape of the network output: (channels, height, width) of the handled images\n",
    " - nhid = parameters in the latent space (mean and logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa9a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, nhid=16):\n",
    "        super(VAE_decoder, self).__init__()\n",
    "        flattened_size = torch.Size(shape).numel() # original input shape\n",
    "        self.shape = shape\n",
    "        self.decode = nn.Sequential(\n",
    "            MLP([nhid, 400, 400, flattened_size], last_activation=False),\n",
    "            #MLP([nhid, 64, 128, 256, flattened_size], last_activation=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.invTrans = Compose([Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    # Decoder maps the latent variables Z to a reconstructed original input data  \n",
    "    def forward(self, z, y=None):\n",
    "        if y is None:\n",
    "            return self.invTrans(self.decode(z).view(-1, *self.shape))\n",
    "        else:\n",
    "            return self.invTrans(self.decode(torch.cat((z, y), dim=1)).view(-1, *self.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d674f74",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Variational AutoEncoder module and loss\n",
    "Fully connected Variational Autoencoder\n",
    "\n",
    "#### Input parameters:\n",
    " - shape = shape of each input sample (channels, height, width) of the handled images\n",
    " - nhid = parameters in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802b505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### base abstract class for generators\n",
    "class Generator(BaseModel):\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, batch_size=None, condition=None):\n",
    "        \"\"\"\n",
    "        Lets the generator sample random samples.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class VAE_model(Generator, nn.Module):\n",
    "\n",
    "    def __init__(self, shape, nhid=16, device=\"cpu\"):\n",
    "        \n",
    "        super(VAE_model, self).__init__()\n",
    "        self.dim = nhid\n",
    "        self.device = device\n",
    "        # ENCODER\n",
    "        self.encoder = VAE_encoder(shape, latent_dim=128)\n",
    "        # parameters in the latent space\n",
    "        self.calc_mean = MLP([128, nhid], last_activation=False)\n",
    "        self.calc_logvar = MLP([128, nhid], last_activation=False)\n",
    "        # DECODER\n",
    "        self.decoder = VAE_decoder(shape, nhid)\n",
    "\n",
    "    # Get features for encoder part given input x\n",
    "    def get_features(self, x):\n",
    "\n",
    "        return self.encoder(x)\n",
    "\n",
    "    # Random samples generator\n",
    "    def generate(self, batch_size=None): # :param batch_size = samples to generate\n",
    "\n",
    "        z = (torch.randn((batch_size, self.dim)).to(self.device) # batch of samples of size \"batch_size\"\n",
    "            if batch_size\n",
    "            else torch.randn((1, self.dim)).to(self.device))     # if batch_size = None, single sample\n",
    "        \n",
    "        res = self.decoder(z)\n",
    "        \n",
    "        if not batch_size:\n",
    "            res = res.squeeze(0)\n",
    "        return res\n",
    "    \n",
    "    # Reparametrization trick (Z = mean + eps*sigma)\n",
    "    def sampling(self, mean, logvar):\n",
    "        \n",
    "        eps = torch.randn(mean.shape).to(self.device)\n",
    "        sigma = 0.5 * torch.exp(logvar)\n",
    "        return mean + eps * sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # map input samples to latent distribution parameters\n",
    "        representations = self.encoder(x)\n",
    "        mean, logvar = self.calc_mean(representations), self.calc_logvar(representations)\n",
    "        # randomly samples similar points from the latent space\n",
    "        z = self.sampling(mean, logvar)\n",
    "        # outputs the reconstructed samples from the latent space points\n",
    "        return self.decoder(z), mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0084a3d",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Loss function of VAE (*theory:* https://gregorygundersen.com/blog/2018/04/29/reparameterization/)\n",
    "\n",
    "The parameters of the model are trained via two loss functions:\n",
    " - **Reconstruction loss:** forcing the decoded samples to match the initial inputs\n",
    " - **Variational loss:** KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term.\n",
    "\n",
    "#### Input parameters:\n",
    " - X = input batch\n",
    " - forward_output = [reconstructed input after autoencoder, mean of the VAE output distribution, logvar of the VAE output distribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb574160",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss = nn.MSELoss(reduction=\"sum\")# Criterion that measures mean squared error (L2 norm)\n",
    "\n",
    "def VAE_loss(X, forward_output):\n",
    "\n",
    "    X_hat, mean, logvar = forward_output\n",
    "    reconstruction_loss = MSE_loss(X_hat, X)\n",
    "    KL_divergence = 0.5 * torch.sum(-1 - logvar + torch.exp(logvar) + mean ** 2)\n",
    "    return reconstruction_loss + KL_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3a0dc",
   "metadata": {},
   "source": [
    "***\n",
    "List of strings defining what symbols are exported in the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac594027",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"VAE_model\", \"VAE_loss\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
